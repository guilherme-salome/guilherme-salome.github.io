{"cells":[{"cell_type":"markdown","metadata":{},"source":["[Tensorflow](https://www.tensorflow.org) is an open source library for numerical computation.\nTensorflow implements a symbolic math library that is used to build a computation graph.\nThis graph is a representation of the computations we wish to perform.\nWhen we write a graph with TensorFlow it is like writing an equation in paper, there are no actual computations occurring.\nTensorFlow uses the graph to automatically compute the derivatives required for stochastic gradient descent.\nWhen it is time to actually perform the computations, TensorFlow uses algorithms in C and C++ for it.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n# Install\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Tensorflow can be installed with `pip`.\nAfter activating your environment in the terminal, run:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Now `numpy` is available for Python and can be imported with:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["All of the Tensorflow functionality can now be accessed via the object `tensorflow`.\n\nYou will often see the following command being used:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Which makes the `tensorflow` functionality available via `tf`.\nThis is often used because it requires less typing to use numpy.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n# Building the Graph\n\n"]},{"cell_type":"markdown","metadata":{},"source":["First import TensorFlow:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["A default graph which is empty is automatically created when we first import TensorFlow. We can access this default graph via:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We can see what operations are attached to it (none in this case) with:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Remember that when we add operations to this graph (think about the nodes and connections in a neural network graph) nothing is actually being computed, we are just creating the structure of the function.\n\nThe simplest operation we can add is a constant:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Notice that by calling `tf.constant`, TensorFlow automatically adds a constant to the default graph, which we can see was added with the `get_operations` method.\nWe did not need to explicitly tell TensorFlow which graph to alter.\nWe can specify which graph to alter, but usually the default graph will suffice.\n\nEach operation in TensorFlow takes an input and generates an output.\nThink about the signal $s_k$ in the neural network graph. It takes as input the features $x_1,\\dots,x_n$, and outputs the value of the signal $s_k$.\nA constant has no inputs, but only an output.\nWe can see those values via:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The output has a special type:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The type is `tf.Tensor`, which can represent data in ascalar, vector, matrix or a multidimensional array.\nThe `shape` is `()` which implies this tensor represents a constant.\n\nAll tensors are outputs of some operation in the graph.\nIn this case, the tensor above is the output of a \"constant operation\".\n\nLet's add in another constant:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Now there are two operations in the graph.\nNotice each has a name, \"Const\" and \"Const<sub>1</sub>\".\nAlso observe the types are `tf.Operation`.\n\nLet's create a new operation that will sum up the two constants.\nFirst, we need to give names to the outputs of the two constants, so that the addition knows what to add:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The naming of the tensors follows `operation_name:output_number`.\nSo \"Const:0\" means the 0th output (first output) coming out of the operation \"Const\".\n\nWe can combine these outputs:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Important, when we create a new operation and assign it to a variable, like `sum_tensor` above, the variable gets the output of the operation, not the operation itself. That is, `sum_tensor` is already the output of `tf.add`.\n\nWe can verify that the addition operation takes two inputs and generates one output:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["\n# Running the Computations\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's now run this computation.\nTo do so, we need to create a `Session` object.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The Session object stores a reference to the graph we just constructed, determines how much memory it will need and sets up some other configurations.\n\nWe can see that the default graph is indeed being used via:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["There is a method `Session.run` which sends the graph to the computation engine to be executed.\nThe method takes as input the tensor you want to compute.\nIf we want to compute the tensor of the addition, we would run:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["You can also pass a list of tensors to compute.\nThis is more efficient, because each time we call the `Session.run` method all computations are performed again.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["After we are done with the computations, we can close the session to free resources:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["\n# Placeholders\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The graph we built uses the same two constant values as inputs to the addition operation.\nLet's modify it so that it can take variables as inputs.\nIn tensorflow variables are created with `tf.placeholder`:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The command above adds a placeholder to the default graph.\nIt takes as input the type of the variable, in this case it is a 32 bytes float.\nOther possible types are: `tf.int32` and `tf.bool`.\nA complete list is available [here](https://www.tensorflow.org/api_docs/python/tf/DType).\n\nLet's add a second placeholder and create a new addition operation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We can now open a `Session` and `Session.run` the `flexible_addition` tensor to get its value.\nHowever, now we need to pass an additional input, the actual values we want for `x` and `y` at the time of computation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We can also give multiple inputs to be evaluated:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We can fetch the value of a tensor multiple times, and each time we can give a new value to the placeholder.\nIf we give it a list of values, then it will use that list as an input.\n\nNotice that the tensor we created for addition uses the `tf.add` method, not the Python `+` method.\nIn Python, using `+` to \"add\" two lists actually contatenate them:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The `tf.add` implements vector addition (actually tensor addition).\n\nIt is possible to add multiple operations at once:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We can get the value of this tensor for multiple inputs via:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["This operation of opening a `Session` performing a computation and then closing it is recurrent in programming in general.\nPython has a special syntax to reduce the number of lines it takes to write the above:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The `with` takes care of opening the `Session` and then closing it when the block of indented code finished running.\n\nWe can clear the default graph with:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["\n# Variables\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Variables are just like placeholders in the sense that they are filled with different values. Unlike placeholders, however, the value of variables are persistent across calls to `session.run`.\nTensorFlow uses `tf.Variable` to hold the value of parameters over which we optimize the value of some loss function.\n\nThe `tf.Variable` constructor takes one input as argument. This input defines the type of the variable, like an integer, floar or even a list.\nLet's create a new variable:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Notice that 4 new operations were added to the graph. These operations provide ways to assign value to the variable and to read the value from it.\n\nIf we try to run a session to get the value of `x` we will get an error:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The error says we are \"Attempting to use uninitialized value Variable<sub>1</sub>:0\".\nThe value `33` is used to determine the type of the variable, but is not automatically set as the default value of the variable.\nThus, the variable does not have a value when we try `Session.run` on it.\nTo assign the number `33` as the default value of `x` we need to initialize it:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The first call to the initializer sets the default value of the Variable.\nAll variables have an `initialize` method to initialize its default value.\nNow, because `x` is a Variable, when we call `session.run` again its value is still `33`.\nThis contrasts to placeholders.\nA placeholder needs its value to be set every time we call `session.run`.\nWe will use placeholders as the input of the neural network.\n\nThere is a helper function that initializes all of the variables to their default values at once:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The method `tf.global_variables_initializer` initializes all of the variables in the graph.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n# Computing Gradients\n\n"]},{"cell_type":"markdown","metadata":{},"source":["One of the main reasons for using TensorFlow is the ability to automatically compute the gradient of a loss function defined on top of a graph.\n\nTo do so, we use the method `tf.gradients`.\nThis method takes two arguments:\n\n1.  `ys`: a tensor or a list of tensors we would like to derivate\n2.  `xs`: a tensor or a list of tensors to derivate with respect to\n\nThe `tf.gradients` method follows the graph starting at `ys` to compute the derivatives.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Let's execute the graph to get the actual value:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Returns the derivative of $x^2$ with respect to $x$ evaluated at $x=10$.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n# Optimization\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We will now implement a linear regression using stochastic gradient descent with TensorFlow.\n\nLet's generate some fake data:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Let's divide the data into training, validation and test sets:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We will use the training data to estimate the parameters, and the validate data to estimate the out of sample loss as we improve the parameter estimates.\nNow, let's build the linear model graph:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["The definition of the placeholder `x_input` takes two arguments: the data type (float) and the shape.\nThe shape is `None` lines by 10 columns, and the `None` tells TensorFlow that the number of lines can be anything.\n\nIf we pass in a vector for `x_input`, then we can obtain the value of the tensor `y_pred`:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["If multiples lines are passed to `x_input`, then we get an array of predictions for $y$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Now, let's add the loss function to the graph:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We can now compute the mean squared error on the training set:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["To obtain the value of the parameters (betas) we will use stochastic gradient descent.\nTensorFlow has us covered with the operation `tf.train.GradientDescentOptimizer`.\nWe can use it to directly compute one step of the gradient descent.\nThe gradient descent method takes as input the learning rate, and provides a method called `minimize`.\nThis `minimize` method takes as input a tensor representing the loss function, and it then takes one step on the gradient descent algorithm using the gradient values of the loss function.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Each time we fetch the operation above, it takes a step following the gradient descent evaluated on whatever data was passed as input to the model.\nBecause we do not have a lot of data, we can run the gradient descent using the entire training set.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["We can evaluate how well this model does on the test set.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["If we were to implement the stochastic gradient descent, then we would feed a single random data point and then update the weights with the gradient descent on the loss function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["\n# Neural Network\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's now use the same data to estimate a simple neural network.\nFirst, reset the default graph:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Then, create the 1st layer of the neural network:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Let's add a final layer:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Create the loss function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Create the gradient descent optimizer:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Minimize the loss function to estimate the parameters:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}